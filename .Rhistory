ggplot(data=SPdata, mapping = aes(x = Size_base)) +
geom_histogram() +
facet_wrap(~Date)
#prep data by making a summary of counts
PDcounts <- PDdata %>%
group_by(Site,Date,Rod_no, Tile_position) %>%
summarise(count=n())
PDcounts
countM <- gamm4(count ~ s(Date), data=PDcounts, family=poisson, random = ~ (1|Site/Rod_no/Tile_position))
#trying to get to run
str(PDcounts)
countM <- gamm4(as.vectore(count) ~ s(Date), data=PDcounts, family=poisson, random = ~ (1|Site/Rod_no/Tile_position))
countM <- gamm4(as.vector(count) ~ s(Date), data=PDcounts, family=poisson, random = ~ (1|Site/Rod_no/Tile_position))
str(PDcounts)
countM <- gamm4(count ~ s(as.numeric(Date)), data=PDcounts, family=poisson, random = ~ (1|Site/Rod_no/Tile_position))
as.numeric(PDcounts$Date)
countM <- gamm4(count ~ s(as.numeric(Date), k=3), data=PDcounts, family=poisson, random = ~ (1|Site/Rod_no/Tile_position))
#trying to get to run
str(PDcounts)
PDcounts$Site <- as.factor(PDcounts$Sitee)
PDcounts$Site <- as.factor(PDcounts$Site)
countM <- gamm4(count ~ s(as.numeric(Date), k=3),
data=PDcounts, family=poisson,
random = ~ (1|Site/Rod_no/Tile_position))
?s
PDcounts$Site <- as.factor(PDcounts$Site)
countM <- gamm4(count ~ s(as.numeric(Date), k=4),
data=PDcounts, family=poisson,
random = ~ (1|Site/Rod_no/Tile_position))
countM
plot(countM)
plot(countM$gam)
summary(countM$mer)
plot(countM$gam)
?numeric
?date
?Date
library(tidyverse)
library(lubridate)
library(dplyr)
library(gamm4)
setwd("/home/timothy/Dropbox/Tim/Post-doc/Review & editing/Cynthia_stats")
# Exploring data ----------------------------------------------------------
corals<-read_csv("my_corals_data.csv") %>%
mutate(Date=dmy(Date)) %>%
mutate(Species=as_factor(Species))  %>%
mutate(Rod_no=as_factor(Rod_no))  %>%
mutate(Tile_position=as.factor(Tile_position))
summary(corals)
#filter to Pocillopora damicornis - the most frequent species
PDdata<-filter(corals, Species == "Pocillopora damicornis")
ggplot(data=PDdata, mapping = aes(x = Size_base)) +
geom_histogram() +
facet_wrap(~Date)
#look at Stylophora too
SPdata<-filter(corals, Species == "Stylophora pistillata")
ggplot(data=SPdata, mapping = aes(x = Size_base)) +
geom_histogram() +
facet_wrap(~Date)
#prep data by making a summary of counts
PDcounts <- PDdata %>%
group_by(Site,Date,Rod_no, Tile_position) %>%
summarise(count=n())
countM <- gamm4(count ~ s(as.numeric(Date), k=4),
data=PDcounts, family=poisson,
random = ~ (1|Site/Rod_no/Tile_position))
PDcounts$dateFromStart <- as.numeric(PDcounts$dateFromStart) - min(PDcounts$dateFromStart)
min(PDcounts$dateFromStart)
PDcounts$dateFromStart <- as.numeric(PDcounts$Date) - min(PDcounts$Date)
PDcounts$dateFromStart <- as.numeric(PDcounts$Date - min(PDcounts$Date))
PDcounts$dateFromStart
str(dateFromStart)
table(PDcounts$dateFromStart)
countM <- gamm4(count ~ s(as.numeric(dateFromStart)),
data=PDcounts, family=poisson,
random = ~ (1|Site/Rod_no/Tile_position))
#TIM: We also have to add a k=n argument to the s() function to restrict how wiggly
# the spline can be, because there are only five unique date values, meaning a
# maximum of 4 knots.
countM <- gamm4(count ~ s(as.numeric(dateFromStart), k=4),
data=PDcounts, family=poisson,
random = ~ (1|Site/Rod_no/Tile_position))
#TIM: The model now runs, and we can examine the "across date" effect like this:
plot(countM$gam)
countML <- glmer(count ~ dateFromStart + (1|Site/Rod_no/Tile_position),
data=PDcounts, family=poisson)
summary(countML)
library(DHARMa)
#TIM: DHARMa relies on simulating model residuals and comparing them to what
# you actually observed
countSimRes <- simulateResiduals(countML)
# first test for overdispersion, given this is Poisson model
testDispersion(countSimRes)
plot(countSimRes)
#TIM: Temporal autocorrelation?
testTemporalAutocorrelation(countSimRes,
time=PDcounts$dateFromStart)
#TIM: Temporal autocorrelation?
testTemporalAutocorrelation(countSimRes,
time=PDcounts$dateFromStart + rnorm(nrow(PDcounts),
0, 1e-6))
countML <- glmer(count ~ as.factor(dateFromStart) + (1|Site/Rod_no/Tile_position),
data=PDcounts, family=poisson)
summary(countML)
#TIM: DHARMa relies on simulating model residuals and comparing them to what
# you actually observed
countSimRes <- simulateResiduals(countML)
#TIM: first test for overdispersion, given this is Poisson model
testDispersion(countSimRes)
#TIM: Temporal autocorrelation?
testTemporalAutocorrelation(countSimRes,
time=PDcounts$dateFromStart + rnorm(nrow(PDcounts),
0, 1e-6))
?dwtest
#TIM: Temporal autocorrelation. This works via a Durbin-Watson test (dwtest function
# which uses a rank order, so observations need to be unique in time. I add a
# tiny bit of random noise to my dates to make them unique
testTemporalAutocorrelation(countSimRes,
time=PDcounts$dateFromStart + rnorm(nrow(PDcounts),
0, 1e-6))
#TIM: Temporal autocorrelation. This works via a Durbin-Watson test (dwtest function
# which uses a rank order, so observations need to be unique in time. I add a
# tiny bit of random noise to my dates to make them unique
testTemporalAutocorrelation(countSimRes,
time=PDcounts$dateFromStart + rnorm(nrow(PDcounts),
0, 1e-6))
#TIM: Temporal autocorrelation. This works via a Durbin-Watson test (dwtest function
# which uses a rank order, so observations need to be unique in time. I add a
# tiny bit of random noise to my dates to make them unique
testTemporalAutocorrelation(countSimRes,
time=PDcounts$dateFromStart + rnorm(nrow(PDcounts),
0, 1e-6))
#TIM: Temporal autocorrelation. This works via a Durbin-Watson test (dwtest function
# which uses a rank order, so observations need to be unique in time. I add a
# tiny bit of random noise to my dates to make them unique
testTemporalAutocorrelation(countSimRes,
time=PDcounts$dateFromStart + rnorm(nrow(PDcounts),
0, 1e-6))
#TIM: Temporal autocorrelation. This works via a Durbin-Watson test (dwtest function
# which uses a rank order, so observations need to be unique in time. I add a
# tiny bit of random noise to my dates to make them unique
testTemporalAutocorrelation(countSimRes,
time=PDcounts$dateFromStart + rnorm(nrow(PDcounts),
0, 1e-6))
#TIM: Temporal autocorrelation. This works via a Durbin-Watson test (dwtest function
# which uses a rank order, so observations need to be unique in time. I add a
# tiny bit of random noise to my dates to make them unique
testTemporalAutocorrelation(countSimRes,
time=PDcounts$dateFromStart + rnorm(nrow(PDcounts),
0, 1e-6))
#TIM: Temporal autocorrelation. This works via a Durbin-Watson test (dwtest function
# which uses a rank order, so observations need to be unique in time. I add a
# tiny bit of random noise to my dates to make them unique
testTemporalAutocorrelation(countSimRes,
time=PDcounts$dateFromStart + rnorm(nrow(PDcounts),
0, 1e-6))
#TIM: Temporal autocorrelation. This works via a Durbin-Watson test (dwtest function
# which uses a rank order, so observations need to be unique in time. I add a
# tiny bit of random noise to my dates to make them unique
testTemporalAutocorrelation(countSimRes,
time=PDcounts$dateFromStart + rnorm(nrow(PDcounts),
0, 1e-6))
#TIM: Temporal autocorrelation. This works via a Durbin-Watson test (dwtest function
# which uses a rank order, so observations need to be unique in time. I add a
# tiny bit of random noise to my dates to make them unique
testTemporalAutocorrelation(countSimRes,
time=PDcounts$dateFromStart + rnorm(nrow(PDcounts),
0, 1e-6))
a <- testTemporalAutocorrelation(countSimRes,
time=PDcounts$dateFromStart + rnorm(nrow(PDcounts),
0, 1e-6))
tempTest <- sapply(1:999, function(n){
a <- testTemporalAutocorrelation(countSimRes,
time=PDcounts$dateFromStart + rnorm(nrow(PDcounts),
0, 1e-6))
return(a$p.value)
})
tempTest
hist(tempTest)
#TIM: you can treat this as a continuous fixed effect
countML <- glmer(count ~ dateFromStart + (1|Site/Rod_no/Tile_position),
data=PDcounts, family=poisson)
tempTest <- sapply(1:99, function(n){
a <- testTemporalAutocorrelation(countSimRes,
time=PDcounts$dateFromStart + rnorm(nrow(PDcounts),
0, 1e-6))
return(a$p.value)
})
hist(tempTest)
source("~/Dropbox/Tim/Post-doc/Review & editing/Cynthia_stats/Cynthia_exploringKimData.R")
rm(list=ls())
rm(list=ls())
library(readxl)
library(d3r)
library(jsonlite)
setwd("/home/timothy/Dropbox/Tim/CV/collabNetwork")
myName = "Timothy L. Staples"
myPapers <- read.csv("wos2.csv", stringsAsFactors = FALSE)
myCoAuth <- sort(unique(unlist(strsplit(myPapers$Author.Full.Names, "; ",))))
paste0("AU = (", paste0(myCoAuth, collapse = ") OR ("), ")")
# cycle through wos subfolder to import 500 paper blocks
wosFiles <- list.files(path="./wos", include.dirs=TRUE, pattern=".xls")
coAuthPapers <- do.call("rbind", lapply(1:length(wosFiles), function(n){
temp <- as.data.frame(read_excel(paste0("./wos/", wosFiles[n])),
stringAsFactors=FALSE)
authorList <- strsplit(temp$`Author Full Names`, "; ")
do.call("rbind", lapply(1:length(authorList), function(n1){
data.frame(auth = authorList[[n1]],
pID = ((n-1)*500) + n1,
pubYear = temp$`Publication Year`[n1])
}))
}))
coAuthPapers$surname <- substr(coAuthPapers$auth,
1, regexpr(", ", coAuthPapers$auth)-1)
coAuthPapers$first <- substr(coAuthPapers$auth,
regexpr(", ", coAuthPapers$auth)+2,
nchar(as.character(coAuthPapers$auth)))
coAuthPapers$full <- paste0(coAuthPapers$first, " ", coAuthPapers$surname)
# make "me" the year of publication
head(coAuthPapers)
# now make a co-author table, only including my co-authors
coAuthPapers <- droplevels(coAuthPapers[coAuthPapers$auth %in% myCoAuth,])
coAuthPapers[coAuthPapers$full  %in% c(myName, "Timothy Staples"),]
# now make me a year
myPubYears <- sort(unique(coAuthPapers$pubYear[coAuthPapers$full  %in% c(myName, "Timothy Staples")]))
coAuthPapers$full[coAuthPapers$full %in% c(myName, "Timothy Staples")] = coAuthPapers$pubYear[coAuthPapers$full  %in% c(myName, "Timothy Staples")]
coAuthMat <- do.call("rbind",
tapply(coAuthPapers$full,
coAuthPapers$pID,
function(x){expand.grid(x, x)}, simplify=FALSE))
coAuthMat <- table(coAuthMat$Var1, coAuthMat$Var2)
aTab <- data.frame(source = rep(rownames(coAuthMat), ncol(coAuthMat)),
target = rep(colnames(coAuthMat), each=nrow(coAuthMat)),
count=as.vector(coAuthMat), stringsAsFactors = FALSE)
for(n in 1:(length(myPubYears)-1)){
aTab$count[aTab$source %in% myPubYears[n] &
aTab$target %in% myPubYears[n+1]] = 1
}
aTab <- aTab[aTab$source != aTab$target,]
aTab <- aTab[aTab$count > 0,]
aTab$primary <- ifelse((aTab$source %in% myPubYears | aTab$target %in% myPubYears),
"#324158", "#8C96A6")
aTab$width <- ifelse(aTab$source %in% myPubYears | aTab$target %in% myPubYears, 2, 0.5)
aTab$width[aTab$source %in% myPubYears & aTab$target %in% myPubYears] = 7.5
countWithMe <- sapply(split(aTab, f=aTab$target), function(x){
if(x$target %in% myPubYears){return(4)}
sum(x$count[x$source %in% c(myPubYears)])
})
# convert table into graph
library(igraph)
network=graph_from_data_frame(d=aTab, directed=F)
V(network)$count = countWithMe[match(V(network)$name, names(countWithMe))]
V(network)$count <- ifelse(V(network)$count == 0, 5, 3.5*V(network)$count)
# V(network)$count[V(network)$name == myName] = 15
V(network)$primary = "#324158"
V(network)$primary[V(network)$name %in% myPubYears] = "white"
V(network)$label = V(network)$name
V(network)$shadow = "rgb(255, 255, 255) 2px 0px 0px, rgb(255, 255, 255) 1.75517px 0.958851px 0px, rgb(255, 255, 255) 1.0806px 1.68294px 0px, rgb(255, 255, 255) 0.141474px 1.99499px 0px, rgb(255, 255, 255) -0.832294px 1.81859px 0px, rgb(255, 255, 255) -1.60229px 1.19694px 0px, rgb(255, 255, 255) -1.97999px 0.28224px 0px, rgb(255, 255, 255) -1.87291px -0.701566px 0px, rgb(255, 255, 255) -1.30729px -1.51361px 0px, rgb(255, 255, 255) -0.421592px -1.95506px 0px, rgb(255, 255, 255) 0.567324px -1.91785px 0px, rgb(255, 255, 255) 1.41734px -1.41108px 0px, rgb(255, 255, 255) 1.92034px -0.558831px 0px"
V(network)$shadow[V(network)$name %in% myPubYears] = "rgb(50, 65, 88) 2px 0px 0px, rgb(50, 65, 88) 1.75517px 0.958851px 0px, rgb(50, 65, 88) 1.0806px 1.68294px 0px, rgb(50, 65, 88) 0.141474px 1.99499px 0px, rgb(50, 65, 88) -0.832294px 1.81859px 0px, rgb(50, 65, 88) -1.60229px 1.19694px 0px, rgb(50, 65, 88) -1.97999px 0.28224px 0px, rgb(50, 65, 88) -1.87291px -0.701566px 0px, rgb(50, 65, 88) -1.30729px -1.51361px 0px, rgb(50, 65, 88) -0.421592px -1.95506px 0px, rgb(50, 65, 88) 0.567324px -1.91785px 0px, rgb(50, 65, 88) 1.41734px -1.41108px 0px, rgb(50, 65, 88) 1.92034px -0.558831px 0px"
# add coordinates for each year
yearVs <- !is.na(as.numeric(V(network)$name))
yearYs <- seq(-length(yearVs), length(yearVs), len=sum(yearVs))
V(network)$cat = 0
V(network)$cat[yearVs] = yearYs
V(network)$catStrength = as.numeric(yearVs)
url <- read.csv("url.csv")
V(network)$url = NA
V(network)$url[match(url$name,
V(network)$name)] = as.character(url$url)
V(network)$hasurl <- !is.na(V(network)$url)
V(network)$offlabel = V(network)$name
V(network)$offlabel[V(network)$name %in% myCoAuth]=""
V(network)$offlabel[match(url$name,
V(network)$name)]= as.character(url$label)
V(network)$width = 0
V(network)$width[V(network)$name %in% myPubYears] = 5
V(network)$strokeCol = "#324158"
V(network)$strokeCol[V(network)$name %in% myPubYears] = "#324158"
plot(network)
rm(list=ls())
library(readxl)
library(d3r)
library(jsonlite)
setwd("/home/timothy/Dropbox/Tim/CV/collabNetwork")
myName = "Timothy L. Staples"
myPapers <- read.csv("wos2.csv", stringsAsFactors = FALSE)
myCoAuth <- sort(unique(unlist(strsplit(myPapers$Author.Full.Names, "; ",))))
paste0("AU = (", paste0(myCoAuth, collapse = ") OR ("), ")")
# cycle through wos subfolder to import 500 paper blocks
wosFiles <- list.files(path="./wos", include.dirs=TRUE, pattern=".xls")
coAuthPapers <- do.call("rbind", lapply(1:length(wosFiles), function(n){
temp <- as.data.frame(read_excel(paste0("./wos/", wosFiles[n])),
stringAsFactors=FALSE)
authorList <- strsplit(temp$`Author Full Names`, "; ")
do.call("rbind", lapply(1:length(authorList), function(n1){
data.frame(auth = authorList[[n1]],
pID = ((n-1)*500) + n1,
pubYear = temp$`Publication Year`[n1])
}))
}))
coAuthPapers$surname <- substr(coAuthPapers$auth,
1, regexpr(", ", coAuthPapers$auth)-1)
coAuthPapers$first <- substr(coAuthPapers$auth,
regexpr(", ", coAuthPapers$auth)+2,
nchar(as.character(coAuthPapers$auth)))
coAuthPapers$full <- paste0(coAuthPapers$first, " ", coAuthPapers$surname)
coAuthPapers
myPapers <- read.csv("wos2.csv", stringsAsFactors = FALSE)
myCoAuth <- sort(unique(unlist(strsplit(myPapers$Author.Full.Names, "; ",))))
paste0("AU = (", paste0(myCoAuth, collapse = ") OR ("), ")")
# cycle through wos subfolder to import 500 paper blocks
wosFiles <- list.files(path="./wos", include.dirs=TRUE, pattern=".xls")
myCoAuth
myPapers$Author.Full.Names
coAuthPapers <- myPapers
coAuthPapers$surname <- substr(coAuthPapers$auth,
1, regexpr(", ", coAuthPapers$auth)-1)
coAuthPapers$auth
coAuthPapers
authorList <- strsplit(myPapers$`Author Full Names`, "; ")
myPapers$`Author Full Names`
authorList <- strsplit(myPapers$Author.Full.Names, "; ")
authorList
coAuthPapers <- strsplit(myPapers$Author.Full.Names, "; ")
authorList <- strsplit(myPapers$Author.Full.Names, "; ")
coAuthPapers <- do.call("rbind", lapply(1:length(authorList), function(n1){
data.frame(auth = authorList[[n1]],
pID = ((n-1)*500) + n1,
pubYear = temp$`Publication Year`[n1])
}))
coAuthPapers <- do.call("rbind", lapply(1:length(authorList), function(n1){
data.frame(auth = authorList[[n1]],
pID = n1,
pubYear = temp$`Publication Year`[n1])
}))
authorList <- strsplit(myPapers$Author.Full.Names, "; ")
coAuthPapers <- do.call("rbind", lapply(1:length(authorList), function(n1){
data.frame(auth = authorList[[n1]],
pID = n1,
pubYear = myPapers$`Publication Year`[n1])
}))
authorList <- strsplit(myPapers$Author.Full.Names, "; ")
coAuthPapers <- do.call("rbind", lapply(1:length(authorList), function(n1){
data.frame(auth = authorList[[n1]],
pID = n1,
pubYear = myPapers$Publication.Year[n1])
}))
coAuthPapers
table(coAuthPapers$auth)
coAuthPapers$surname <- substr(coAuthPapers$auth,
1, regexpr(", ", coAuthPapers$auth)-1)
coAuthPapers$first <- substr(coAuthPapers$auth,
regexpr(", ", coAuthPapers$auth)+2,
nchar(as.character(coAuthPapers$auth)))
coAuthPapers$full <- paste0(coAuthPapers$first, " ", coAuthPapers$surname)
coAuthPapers
# now make a co-author table, only including my co-authors
coAuthPapers <- droplevels(coAuthPapers[coAuthPapers$auth %in% myCoAuth,])
coAuthPapers[coAuthPapers$full  %in% c(myName, "Timothy Staples"),]
# now make me a year
myPubYears <- sort(unique(coAuthPapers$pubYear[coAuthPapers$full  %in% c(myName, "Timothy Staples")]))
myPubYears
coAuthPapers$full[coAuthPapers$full %in% c(myName, "Timothy Staples")] = coAuthPapers$pubYear[coAuthPapers$full  %in% c(myName, "Timothy Staples")]
coAuthPapers
coAuthMat <- do.call("rbind",
tapply(coAuthPapers$full,
coAuthPapers$pID,
function(x){expand.grid(x, x)}, simplify=FALSE))
coAuthMat <- table(coAuthMat$Var1, coAuthMat$Var2)
aTab <- data.frame(source = rep(rownames(coAuthMat), ncol(coAuthMat)),
target = rep(colnames(coAuthMat), each=nrow(coAuthMat)),
count=as.vector(coAuthMat), stringsAsFactors = FALSE)
for(n in 1:(length(myPubYears)-1)){
aTab$count[aTab$source %in% myPubYears[n] &
aTab$target %in% myPubYears[n+1]] = 1
}
aTab <- aTab[aTab$source != aTab$target,]
aTab <- aTab[aTab$count > 0,]
aTab$primary <- ifelse((aTab$source %in% myPubYears | aTab$target %in% myPubYears),
"#324158", "#8C96A6")
aTab$width <- ifelse(aTab$source %in% myPubYears | aTab$target %in% myPubYears, 2, 0.5)
aTab$width[aTab$source %in% myPubYears & aTab$target %in% myPubYears] = 7.5
countWithMe <- sapply(split(aTab, f=aTab$target), function(x){
if(x$target %in% myPubYears){return(4)}
sum(x$count[x$source %in% c(myPubYears)])
})
# convert table into graph
library(igraph)
network=graph_from_data_frame(d=aTab, directed=F)
plot(network)
library(igraph)
network=graph_from_data_frame(d=aTab, directed=F)
V(network)$count = countWithMe[match(V(network)$name, names(countWithMe))]
V(network)$count <- ifelse(V(network)$count == 0, 5, 3.5*V(network)$count)
# V(network)$count[V(network)$name == myName] = 15
V(network)$primary = "#324158"
V(network)$primary[V(network)$name %in% myPubYears] = "white"
V(network)$label = V(network)$name
V(network)$shadow = "rgb(255, 255, 255) 2px 0px 0px, rgb(255, 255, 255) 1.75517px 0.958851px 0px, rgb(255, 255, 255) 1.0806px 1.68294px 0px, rgb(255, 255, 255) 0.141474px 1.99499px 0px, rgb(255, 255, 255) -0.832294px 1.81859px 0px, rgb(255, 255, 255) -1.60229px 1.19694px 0px, rgb(255, 255, 255) -1.97999px 0.28224px 0px, rgb(255, 255, 255) -1.87291px -0.701566px 0px, rgb(255, 255, 255) -1.30729px -1.51361px 0px, rgb(255, 255, 255) -0.421592px -1.95506px 0px, rgb(255, 255, 255) 0.567324px -1.91785px 0px, rgb(255, 255, 255) 1.41734px -1.41108px 0px, rgb(255, 255, 255) 1.92034px -0.558831px 0px"
V(network)$shadow[V(network)$name %in% myPubYears] = "rgb(50, 65, 88) 2px 0px 0px, rgb(50, 65, 88) 1.75517px 0.958851px 0px, rgb(50, 65, 88) 1.0806px 1.68294px 0px, rgb(50, 65, 88) 0.141474px 1.99499px 0px, rgb(50, 65, 88) -0.832294px 1.81859px 0px, rgb(50, 65, 88) -1.60229px 1.19694px 0px, rgb(50, 65, 88) -1.97999px 0.28224px 0px, rgb(50, 65, 88) -1.87291px -0.701566px 0px, rgb(50, 65, 88) -1.30729px -1.51361px 0px, rgb(50, 65, 88) -0.421592px -1.95506px 0px, rgb(50, 65, 88) 0.567324px -1.91785px 0px, rgb(50, 65, 88) 1.41734px -1.41108px 0px, rgb(50, 65, 88) 1.92034px -0.558831px 0px"
# add coordinates for each year
yearVs <- !is.na(as.numeric(V(network)$name))
yearYs <- seq(-length(yearVs), length(yearVs), len=sum(yearVs))
V(network)$cat = 0
V(network)$cat[yearVs] = yearYs
V(network)$catStrength = as.numeric(yearVs)
url <- read.csv("url.csv")
V(network)$url = NA
V(network)$url[match(url$name,
V(network)$name)] = as.character(url$url)
V(network)$hasurl <- !is.na(V(network)$url)
V(network)$offlabel = V(network)$name
V(network)$offlabel[V(network)$name %in% myCoAuth]=""
V(network)$offlabel[match(url$name,
V(network)$name)]= as.character(url$label)
V(network)$width = 0
V(network)$width[V(network)$name %in% myPubYears] = 5
V(network)$strokeCol = "#324158"
V(network)$strokeCol[V(network)$name %in% myPubYears] = "#324158"
# Transform it in a JSON format for d3.js
data_json <- d3_igraph(network)
# Save this file
write(data_json, "/home/timothy/Dropbox/Tim/CV/collabNetwork/data.json")
rm(list=ls())
library(readxl)
library(d3r)
library(jsonlite)
setwd("/home/timothy/Dropbox/Tim/CV/collabNetwork")
myName = "Timothy L. Staples"
myPapers <- read.csv("wos2.csv", stringsAsFactors = FALSE)
myCoAuth <- sort(unique(unlist(strsplit(myPapers$Author.Full.Names, "; ",))))
paste0("AU = (", paste0(myCoAuth, collapse = ") OR ("), ")")
# cycle through wos subfolder to import 500 paper blocks
wosFiles <- list.files(path="./wos", include.dirs=TRUE, pattern=".xls")
authorList <- strsplit(myPapers$Author.Full.Names, "; ")
coAuthPapers <- do.call("rbind", lapply(1:length(authorList), function(n1){
data.frame(auth = authorList[[n1]],
pID = n1,
pubYear = myPapers$Publication.Year[n1])
}))
coAuthPapers$surname <- substr(coAuthPapers$auth,
1, regexpr(", ", coAuthPapers$auth)-1)
coAuthPapers$first <- substr(coAuthPapers$auth,
regexpr(", ", coAuthPapers$auth)+2,
nchar(as.character(coAuthPapers$auth)))
coAuthPapers$full <- paste0(coAuthPapers$first, " ", coAuthPapers$surname)
head(coAuthPapers)
myPubYears <- sort(unique(coAuthPapers$pubYear[coAuthPapers$full  %in% c(myName, "Timothy Staples")]))
coAuthPapers$full[coAuthPapers$full %in% c(myName, "Timothy Staples")] = coAuthPapers$pubYear[coAuthPapers$full  %in% c(myName, "Timothy Staples")]
coAuthMat <- do.call("rbind",
tapply(coAuthPapers$full,
coAuthPapers$pID,
function(x){expand.grid(x, x)}, simplify=FALSE))
coAuthMat <- table(coAuthMat$Var1, coAuthMat$Var2)
coAuthMat
aTab <- data.frame(source = rep(rownames(coAuthMat), ncol(coAuthMat)),
target = rep(colnames(coAuthMat), each=nrow(coAuthMat)),
count=as.vector(coAuthMat), stringsAsFactors = FALSE)
aTab
unique(coAuthPapers$pubYear)
aTab$source %in% unique(coAuthPapers$pubYear)
#remove any edge not with me
aTab <- aTab[aTab$source %in% unique(coAuthPapers$pubYear) |
aTab$target %in% unique(coAuthPapers$pubYear),]
for(n in 1:(length(myPubYears)-1)){
aTab$count[aTab$source %in% myPubYears[n] &
aTab$target %in% myPubYears[n+1]] = 1
}
aTab <- aTab[aTab$source != aTab$target,]
aTab <- aTab[aTab$count > 0,]
aTab$primary <- ifelse((aTab$source %in% myPubYears | aTab$target %in% myPubYears),
"#324158", "#8C96A6")
aTab$width <- ifelse(aTab$source %in% myPubYears | aTab$target %in% myPubYears, 2, 0.5)
aTab$width[aTab$source %in% myPubYears & aTab$target %in% myPubYears] = 7.5
countWithMe <- sapply(split(aTab, f=aTab$target), function(x){
if(x$target %in% myPubYears){return(4)}
sum(x$count[x$source %in% c(myPubYears)])
})
# convert table into graph
library(igraph)
network=graph_from_data_frame(d=aTab, directed=F)
V(network)$count = countWithMe[match(V(network)$name, names(countWithMe))]
V(network)$count <- ifelse(V(network)$count == 0, 5, 3.5*V(network)$count)
# V(network)$count[V(network)$name == myName] = 15
V(network)$primary = "#324158"
V(network)$primary[V(network)$name %in% myPubYears] = "white"
V(network)$label = V(network)$name
V(network)$shadow = "rgb(255, 255, 255) 2px 0px 0px, rgb(255, 255, 255) 1.75517px 0.958851px 0px, rgb(255, 255, 255) 1.0806px 1.68294px 0px, rgb(255, 255, 255) 0.141474px 1.99499px 0px, rgb(255, 255, 255) -0.832294px 1.81859px 0px, rgb(255, 255, 255) -1.60229px 1.19694px 0px, rgb(255, 255, 255) -1.97999px 0.28224px 0px, rgb(255, 255, 255) -1.87291px -0.701566px 0px, rgb(255, 255, 255) -1.30729px -1.51361px 0px, rgb(255, 255, 255) -0.421592px -1.95506px 0px, rgb(255, 255, 255) 0.567324px -1.91785px 0px, rgb(255, 255, 255) 1.41734px -1.41108px 0px, rgb(255, 255, 255) 1.92034px -0.558831px 0px"
V(network)$shadow[V(network)$name %in% myPubYears] = "rgb(50, 65, 88) 2px 0px 0px, rgb(50, 65, 88) 1.75517px 0.958851px 0px, rgb(50, 65, 88) 1.0806px 1.68294px 0px, rgb(50, 65, 88) 0.141474px 1.99499px 0px, rgb(50, 65, 88) -0.832294px 1.81859px 0px, rgb(50, 65, 88) -1.60229px 1.19694px 0px, rgb(50, 65, 88) -1.97999px 0.28224px 0px, rgb(50, 65, 88) -1.87291px -0.701566px 0px, rgb(50, 65, 88) -1.30729px -1.51361px 0px, rgb(50, 65, 88) -0.421592px -1.95506px 0px, rgb(50, 65, 88) 0.567324px -1.91785px 0px, rgb(50, 65, 88) 1.41734px -1.41108px 0px, rgb(50, 65, 88) 1.92034px -0.558831px 0px"
# add coordinates for each year
yearVs <- !is.na(as.numeric(V(network)$name))
yearYs <- seq(-length(yearVs), length(yearVs), len=sum(yearVs))
V(network)$cat = 0
V(network)$cat[yearVs] = yearYs
V(network)$catStrength = as.numeric(yearVs)
url <- read.csv("url.csv")
V(network)$url = NA
V(network)$url[match(url$name,
V(network)$name)] = as.character(url$url)
V(network)$hasurl <- !is.na(V(network)$url)
V(network)$offlabel = V(network)$name
V(network)$offlabel[V(network)$name %in% myCoAuth]=""
V(network)$offlabel[match(url$name,
V(network)$name)]= as.character(url$label)
V(network)$width = 0
V(network)$width[V(network)$name %in% myPubYears] = 5
V(network)$strokeCol = "#324158"
V(network)$strokeCol[V(network)$name %in% myPubYears] = "#324158"
# Transform it in a JSON format for d3.js
data_json <- d3_igraph(network)
# Save this file
write(data_json, "/home/timothy/Dropbox/Tim/CV/collabNetwork/data.json")
